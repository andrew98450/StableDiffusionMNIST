{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion MNIST By Andrew Huang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers.models import UNet2DModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define VAE Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, latent_dim = 4) -> None:\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, latent_dim, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(latent_dim),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(latent_dim, latent_dim, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(latent_dim),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.decoder_layer = torch.nn.Sequential(\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            torch.nn.Conv2d(latent_dim, latent_dim, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(latent_dim),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            torch.nn.Conv2d(latent_dim, 1, 3, padding=1),\n",
    "            torch.nn.Tanh())\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        encoded = self.encoder_layer(inputs)\n",
    "        decoded = self.decoder_layer(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define StableDiffusion Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableDiffusion(torch.nn.Module):\n",
    "    def __init__(self, t = 50, vae_model = \"./vae_model.pth\", latent_dim = 4) -> None:\n",
    "        super(StableDiffusion, self).__init__()\n",
    "        self.t = t\n",
    "        self.vae_layer = VAE(latent_dim=latent_dim)\n",
    "        self.vae_layer.load_state_dict(torch.load(vae_model))\n",
    "        self.unet_layer = UNet2DModel(\n",
    "            sample_size=8,\n",
    "            in_channels=16, \n",
    "            out_channels=16,\n",
    "            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\"),\n",
    "            up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\"),\n",
    "            block_out_channels=(64, 64),\n",
    "            downsample_type=\"conv\",\n",
    "            upsample_type=\"conv\",\n",
    "            norm_num_groups=32)\n",
    "    \n",
    "    def encoder(self, inputs):\n",
    "        encoded = self.vae_layer.encoder_layer(inputs)\n",
    "        return encoded\n",
    "\n",
    "    def forward(self, inputs, t):\n",
    "        outputs = self.unet_layer(inputs, t).sample\n",
    "        return outputs\n",
    "    \n",
    "    def decoder(self, inputs):\n",
    "        outputs = self.vae_layer.decoder_layer(inputs)\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, sample_size = 25, use_cuda = True):\n",
    "        xt = torch.randn(sample_size, 16, 8, 8).float()\n",
    "        betas = torch.linspace(0.01, 0.2, steps=self.t).float()\n",
    "\n",
    "        if use_cuda:\n",
    "            xt = xt.cuda()\n",
    "            betas = betas.cuda()\n",
    "\n",
    "        alpha = 1 - betas\n",
    "        alpha_hat = torch.cumprod(alpha, dim=0)\n",
    "        sigma = betas.sqrt()\n",
    "        for i in reversed(range(self.t)):\n",
    "            z = torch.randn_like(xt) if i > 0 else torch.zeros_like(xt)\n",
    "            t = torch.full((sample_size,), i).long()\n",
    "            if use_cuda:\n",
    "                t = t.cuda()\n",
    "\n",
    "            lambdas = (1 - alpha[i]) / torch.sqrt(1 - alpha_hat[i])\n",
    "            xt = (1 / torch.sqrt(alpha[i])) * (xt - (lambdas * self(xt, t))) + (z * sigma[i])\n",
    "\n",
    "        xt = self.decoder(xt)\n",
    "        return xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define VAE Hyper-Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr = 0.0002\n",
    "batch_size = 128\n",
    "sample_size = 25\n",
    "iters = 0\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32, 32)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(root=\"./\", train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "vae_model = VAE(latent_dim=16)\n",
    "vae_model = vae_model.cuda()\n",
    "\n",
    "vae_optim = torch.optim.Adam(vae_model.parameters(), lr=lr)\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "vae_summary = SummaryWriter(log_dir=\"./vae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training VAE Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for inputs, _ in dataloader:\n",
    "        inputs = inputs.float().cuda()\n",
    "        \n",
    "        outputs = vae_model(inputs)\n",
    "        loss = mse_loss(outputs, inputs)\n",
    "\n",
    "        vae_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        vae_optim.step()\n",
    "        \n",
    "        if iters % 20 == 0:\n",
    "            print(\"[+] Epoch [%d/%d] Loss: %.4f\" % (epoch+1, epochs, loss))\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs[0:sample_size]\n",
    "                outputs = vae_model(inputs)\n",
    "                fig = plt.figure()\n",
    "                for i in range(sample_size):\n",
    "                    image = 0.5 + outputs[i] * 0.5\n",
    "                    image = image * 255.0\n",
    "                    image = image.squeeze().byte().cpu().numpy()\n",
    "                    plt.subplot(5, 5, i+1)\n",
    "                    plt.imshow(image, cmap=\"gray\")\n",
    "                    plt.axis(\"off\")\n",
    "                #plt.show()\n",
    "                vae_summary.add_scalar(\"Loss\", loss, iters)\n",
    "                vae_summary.add_figure(\"Image\", fig, iters)\n",
    "        \n",
    "        iters += 1\n",
    "        \n",
    "vae_summary.close()\n",
    "\n",
    "vae_model = vae_model.cpu()\n",
    "\n",
    "torch.save(vae_model.state_dict(), \"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define StableDiffusion Hyper-Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "sample_size = 25\n",
    "iters = 0\n",
    "t_step = 50\n",
    "betas = torch.linspace(0.01, 0.2, steps=t_step).float().cuda()\n",
    "alpha = 1 - betas\n",
    "alpha_hat = torch.cumprod(alpha, dim=0)\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32, 32)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(root=\"./\", train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "model = StableDiffusion(t_step, latent_dim=16)\n",
    "model = model.cuda()\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "summary = SummaryWriter(log_dir=\"./stablediffusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training StableDiffusion Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for inputs, _ in dataloader:\n",
    "        inputs = inputs.float().cuda()\n",
    "        \n",
    "        t = torch.randint(low=0, high=t_step, size=(batch_size,)).long().cuda()\n",
    "        \n",
    "        alpha_hats = alpha_hat[t].reshape((-1, 1, 1, 1))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoded = model.encoder(inputs)\n",
    "        eps = torch.randn_like(encoded)\n",
    "        xt_encoded = encoded * torch.sqrt(alpha_hats) + eps * torch.sqrt(1 - alpha_hats)\n",
    "        \n",
    "        eps_outputs = model(xt_encoded, t)\n",
    "        loss = mse_loss(eps_outputs, eps)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if iters % 20 == 0:\n",
    "            print(\"[+] Epoch [%d/%d] Loss: %.4f\" % (epoch+1, epochs, loss))\n",
    "            with torch.no_grad():\n",
    "                outputs = model.sample(sample_size=sample_size, use_cuda=True)\n",
    "                fig = plt.figure()\n",
    "                for i in range(sample_size):\n",
    "                    image = 0.5 + outputs[i] * 0.5\n",
    "                    image = image * 255.0\n",
    "                    image = image.squeeze().byte().cpu().numpy()\n",
    "                    plt.subplot(5, 5, i+1)\n",
    "                    plt.imshow(image, cmap=\"gray\")\n",
    "                    plt.axis(\"off\")\n",
    "                #plt.show()\n",
    "                summary.add_scalar(\"Loss\", loss, iters)\n",
    "                summary.add_figure(\"Image\", fig, iters)\n",
    "        \n",
    "        iters += 1\n",
    "        \n",
    "summary.close()\n",
    "\n",
    "model = model.cpu()\n",
    "\n",
    "torch.save(model.state_dict(), \"stablediffusion_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StableDiffusion(t=50, latent_dim=16)\n",
    "model.load_state_dict(torch.load(\"./stablediffusion_model.pth\"))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images = model.sample(36, use_cuda=False)\n",
    "    for i in range(36):\n",
    "        image = (images[i].clamp(-1., 1.) + 1) / 2\n",
    "        image = image * 255.0\n",
    "        image = image.byte().squeeze().cpu().numpy()\n",
    "        plt.subplot(6, 6, i + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
